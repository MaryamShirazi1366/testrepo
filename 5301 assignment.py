# -*- coding: utf-8 -*-
"""Vikas_assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oxleChLa7sCCtFrgjCREQ2pc9klY7cF0
"""

import pandas as pd
import numpy as np
import datetime as dt
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn import tree
from sklearn import metrics
from sklearn.metrics import confusion_matrix, plot_confusion_matrix, classification_report
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import StratifiedKFold


plt.style.use('seaborn')



df = pd.read_csv("/Users/maryamshirazi/Desktop/5301/crunchbase/crunchbase.csv")
print("Total number of rows and columns:", df.shape)
print()
df.tail(10)

# Checking the missing values
df.isnull().sum()

# Creating a new column for total number of degrees obtained by the founders
df['number_degrees']= df.iloc[:, 10:14].sum(axis=1)
df.head()

df.isnull().sum()

df['age']

df['age_new'] = df['age'] //365
df.head()

df = df[(df['age_new']>=3) & (df['age_new'] <=7)]
df.age_new.unique

# only 11019 startups are between 3 to 7 years old
df.shape

#droping the columns which have more than 50% of missing values
df_clean = df.drop(["products_number", "acquired_companies", "mba_degree","phd_degree", "ms_degree", "other_degree", "age"], axis = 1)
df_clean.isnull().sum()

print(df_clean['average_funded'].mean())
print(df_clean['offices'].mean())
#df_clean['age'].mean()

#replace missing values of the following columns with their average
df_clean[['average_funded', 'offices']] = df_clean[['average_funded','offices']].fillna(df_clean[['average_funded', 'offices']].mean())

df_clean.isnull().sum()

# dropping the rows with missing values which cannot be replaced
df_clean = df_clean.dropna()
df_clean.isnull().sum()

df_clean.info()

df_clean.head()

#converting boolean values to integer to create target variable
df_clean[['ipo', 'is_acquired', 'is_closed']] = df_clean[['ipo', 'is_acquired', 'is_closed']].replace({True : 1, False: 0})
df_clean.head()

#creating the target variable

def success(rows):
    if rows['ipo'] == 1:
        return 1
    elif rows['is_acquired'] == 1:
        return 1
    elif rows['is_acquired'] == 0:
        return 0

df_clean['success'] = df_clean.apply(lambda rows: success(rows), axis=1) 
df_clean.tail(15)

df_clean.info()

df_clean['average_funded'] = df_clean['average_funded'].round(2)
df_clean.head()

plt.figure(figsize=(14, 9))
df_num = df_clean.select_dtypes(exclude = ['object'])
sns.heatmap(data = df_num.corr(), annot = True, cmap = 'icefire');

df_cat = df_clean.select_dtypes(include = ['object'])
df_cat

df_cat_encode = pd.get_dummies(df_cat, drop_first=True)
df_cat_encode

#merging the numeric and categorical dataframes to a clean new dataframe which ia ready for ML
df_new = pd.concat([df_num, df_cat_encode], axis = 1)
df_new = df_new.drop(['ipo', 'is_acquired', 'is_closed'], axis = 1)
df_new.head()

# imbalance in the data
df_new['success'].value_counts()

# Normalizing the data
X = df_new.drop(columns = ['success'], axis = 1)
y = df_new['success']

ss = StandardScaler()
x = ss.fit_transform(X)
x

# Decision Tree

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state = 30)
dtc = DecisionTreeClassifier()
dtc.fit(x_train, y_train)
y_train_pred = dtc.predict(x_train)
y_test_pred = dtc.predict(x_test)
print("Train Accracy:", metrics.accuracy_score(y_train, y_train_pred))
print("Test Accuracy:", metrics.accuracy_score(y_test, y_test_pred))

plot_confusion_matrix(dtc, x_train, y_train, values_format = 'd')
print(classification_report(y_train, y_train_pred))
plt.grid(False)

plot_confusion_matrix(dtc, x_test, y_test, values_format = 'd')
print(classification_report(y_test, y_test_pred))
plt.grid(False)

fpr_un, tpr_un, threshold_un = roc_curve(y_test, y_test_pred)

plt.figure(figsize = (10,4), dpi = 98)
# Plot roc curves
plt.plot([0, 1], [0, 1], label = 'Baseline', linestyle = '--', color= 'red')
plt.plot(fpr_un, tpr_un,color = 'yellow', label = 'Unpruned Decision Tree')
plt.title('ROC curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive rate')
plt.legend(loc='lower right');

"""ROC curve

- The baseline curve (dashed line) indicates that the classifier is not able to distinguish between Positive and Negative class points. Meaning either the classifier is predicting random class or constant class for all the data points.

- The farthest the line from the baseline meaning it is able to it is able to detect more numbers of True positives and True negatives than False negatives and False positives.
"""

print('AUC score for Unpruend Decision Tree Classifier: {0:.3f}'.format(metrics.roc_auc_score(y_test, y_test_pred)))

# Random Forest

rf_clf = RandomForestClassifier()
rf_clf.fit(x_train, y_train)
y_train_pred_rf = rf_clf.predict(x_train)
y_test_pred_rf = rf_clf.predict(x_test)
print("Train Accracy:", metrics.accuracy_score(y_train, y_train_pred))
print("Test Accuracy:", metrics.accuracy_score(y_test, y_test_pred_rf))

plot_confusion_matrix(rf_clf, x_test, y_test, values_format = 'd')
print(classification_report(y_test, y_test_pred_rf))
plt.grid(False)

fpr_un, tpr_un, threshold_un = roc_curve(y_test, y_test_pred)

plt.figure(figsize = (10,4), dpi = 98)
# Plot roc curves
plt.plot([0, 1], [0, 1], label = 'Baseline', linestyle = '--', color= 'red')
plt.plot(fpr_un, tpr_un,color = 'yellow', label = 'Random Forest')
plt.title('ROC curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive rate')
plt.legend(loc='lower right');



""">> ## There is still scope of doing hyper parameter tuning, you can try that and see if the model improves.

However with this data, Decision Tree has performed much better compared to Random Forest, if we see the ROC curve and the confusion matrix. 
But due to the imbalance in the data, there are high chances of overfitting, which can be reducing by balancing techniques
"""

# features that most affect startup success - Random Forest
rf_clf.feature_importances_
feat_importances = pd.Series(rf_clf.feature_importances_, index=X.columns)
feat_importances.nlargest(10).plot(kind='barh')
plt.show()

# features that most affect startup success - Decision tree
dtc.feature_importances_
feat_importances = pd.Series(dtc.feature_importances_, index=X.columns)
feat_importances.nlargest(10).plot(kind='barh')
plt.show()

"""### K-fold cross validation

"""

#pipeline for Decision Tree
pipeline = make_pipeline(StandardScaler(), DecisionTreeClassifier(criterion='gini', max_depth=4))
#
# Pass instance of pipeline and training and test data set
# cv=10 represents the StratifiedKFold with 10 folds
#
scores = cross_val_score(pipeline, X=x_train, y=y_train, cv=10, n_jobs=1)
 
print('Cross Validation accuracy scores: %s' % scores)
 
print('Cross Validation accuracy: %.3f +/- %.3f' % (np.mean(scores),np.std(scores)))

#pipeline for random forest
pipeline = make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=100, max_depth=4))
#
# Pass instance of pipeline and training and test data set
# cv=10 represents the StratifiedKFold with 10 folds
#
scores = cross_val_score(pipeline, X=x_train, y=y_train, cv=10, n_jobs=1)
 
print('Cross Validation accuracy scores: %s' % scores)
 
print('Cross Validation accuracy: %.3f +/- %.3f' % (np.mean(scores),np.std(scores)))